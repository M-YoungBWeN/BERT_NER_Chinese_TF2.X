2024-06-16 21:39:58
++++++++++++++++++++++++++++++++++++++++CONFIGURATION SUMMARY++++++++++++++++++++++++++++++++++++++++
 Status:
     mode                 : train
 ++++++++++++++++++++++++++++++++++++++++
 Datasets:
     datasets         fold: data/example_datasets
     train            file: train.csv
     validation       file: dev.csv
     vocab             dir: data/example_datasets/vocabs
     delimiter            : b
     use  pretrained model: True
     pretrained      model: Bert
     huggingface       tag: bert-base-chinese
     finetune             : True
     use    middle   model: True
     middle          model: bilstm
     checkpoints       dir: checkpoints/0616
     log               dir: data/example_datasets/logs
 ++++++++++++++++++++++++++++++++++++++++
Labeling Scheme:
     label          scheme: BIO
     label           level: 2
     suffixes             : ['ORG', 'PER', 'LOC']
     measuring     metrics: ['precision', 'recall', 'f1', 'accuracy']
 ++++++++++++++++++++++++++++++++++++++++
Model Configuration:
     embedding         dim: 300
     max  sequence  length: 202
     hidden            dim: 128
     filter           nums: 32
     idcnn            nums: 2
     CUDA  VISIBLE  DEVICE: 0
     seed                 : 5
 ++++++++++++++++++++++++++++++++++++++++
 Training Settings:
     epoch                : 1
     batch            size: 4
     dropout              : 0.5
     learning         rate: 5e-05
     optimizer            : Adam
     use               gan: False
     gan            method: fgm
     checkpoint       name: model0616
     max       checkpoints: 3
     print       per_batch: 20
     is     early     stop: True
     patient              : 5
++++++++++++++++++++++++++++++++++++++++CONFIGURATION SUMMARY END++++++++++++++++++++++++++++++++++++++++
label vocab files not exist, building label vocab...
dataManager initialed...
mode: train
loading train data...
loading validation data...
training set size: 23181, validating set size: 4636
++++++++++++++++++++training starting++++++++++++++++++++
epoch:1/1
training batch:    20, loss: 7.36242, precision: 0.000 recall: 0.000 f1: 0.000 accuracy: 0.974 
training batch:    40, loss: 1.87506, precision: 0.000 recall: 0.000 f1: 0.000 accuracy: 0.988 
training batch:    60, loss: 6.89118, precision: 1.000 recall: 0.200 f1: 0.333 accuracy: 0.955 
training batch:    80, loss: 11.41233, precision: 0.250 recall: 0.167 f1: 0.200 accuracy: 0.918 
training batch:   100, loss: 0.72472, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   120, loss: 15.44086, precision: 1.000 recall: 0.400 f1: 0.571 accuracy: 0.939 
training batch:   140, loss: 18.17431, precision: 0.385 recall: 0.455 f1: 0.417 accuracy: 0.900 
training batch:   160, loss: 2.02784, precision: 0.667 recall: 0.500 f1: 0.571 accuracy: 0.996 
training batch:   180, loss: 14.78685, precision: 1.000 recall: 0.714 f1: 0.833 accuracy: 0.916 
training batch:   200, loss: 2.51241, precision: 0.750 recall: 0.600 f1: 0.667 accuracy: 0.989 
training batch:   220, loss: 2.29117, precision: 0.857 recall: 0.857 f1: 0.857 accuracy: 0.995 
training batch:   240, loss: 5.85618, precision: 0.714 recall: 0.833 f1: 0.769 accuracy: 0.967 
