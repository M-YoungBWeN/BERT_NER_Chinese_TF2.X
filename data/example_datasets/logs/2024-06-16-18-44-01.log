2024-06-16 18:44:01
++++++++++++++++++++++++++++++++++++++++CONFIGURATION SUMMARY++++++++++++++++++++++++++++++++++++++++
 Status:
     mode                 : train
 ++++++++++++++++++++++++++++++++++++++++
 Datasets:
     datasets         fold: data/example_datasets
     train            file: train.csv
     validation       file: dev.csv
     vocab             dir: data/example_datasets/vocabs
     delimiter            : b
     use  pretrained model: True
     pretrained      model: Bert
     huggingface       tag: E:\\1_Code\\Transformer\\BERT_Data\\bert230\\bert-base-chinese
     finetune             : True
     use    middle   model: True
     middle          model: bilstm
     checkpoints       dir: checkpoints/0616
     log               dir: data/example_datasets/logs
 ++++++++++++++++++++++++++++++++++++++++
Labeling Scheme:
     label          scheme: BIO
     label           level: 2
     suffixes             : ['ORG', 'PER', 'LOC']
     measuring     metrics: ['precision', 'recall', 'f1', 'accuracy']
 ++++++++++++++++++++++++++++++++++++++++
Model Configuration:
     embedding         dim: 300
     max  sequence  length: 202
     hidden            dim: 8
     filter           nums: 32
     idcnn            nums: 2
     CUDA  VISIBLE  DEVICE: 0
     seed                 : 5
 ++++++++++++++++++++++++++++++++++++++++
 Training Settings:
     epoch                : 1
     batch            size: 1
     dropout              : 0.5
     learning         rate: 5e-05
     optimizer            : Adam
     use               gan: False
     gan            method: fgm
     checkpoint       name: model0616
     max       checkpoints: 3
     print       per_batch: 20
     is     early     stop: True
     patient              : 5
++++++++++++++++++++++++++++++++++++++++CONFIGURATION SUMMARY END++++++++++++++++++++++++++++++++++++++++
label vocab files not exist, building label vocab...
dataManager initialed...
mode: train
loading train data...
loading validation data...
training set size: 23181, validating set size: 4636
++++++++++++++++++++training starting++++++++++++++++++++
epoch:1/1
training batch:    20, loss: 17.92465, precision: 0.000 recall: 0.000 f1: 0.000 accuracy: 0.765 
training batch:    40, loss: 26.59566, precision: 0.000 recall: 0.000 f1: 0.000 accuracy: 0.911 
training batch:    60, loss: 41.66858, precision: 0.000 recall: 0.000 f1: 0.000 accuracy: 0.899 
training batch:    80, loss: 26.27625, precision: 0.000 recall: 0.000 f1: 0.000 accuracy: 0.955 
training batch:   100, loss: 4.34764, precision: 0.000 recall: 0.000 f1: 0.000 accuracy: 1.000 
training batch:   120, loss: 15.29034, precision: 0.000 recall: 0.000 f1: 0.000 accuracy: 1.000 
